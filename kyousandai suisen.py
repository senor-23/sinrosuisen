import streamlit as st
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD

# ===============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===============================
df = pd.read_excel("excel1.xlsx", sheet_name="Sheet1")

# ===============================
# åˆ—å®šç¾©
# ===============================
bunkei_courses = [
    'çµŒæ¸ˆ/çµŒæ¸ˆ', 'çµŒå–¶/ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'æ³•/æ³•å¾‹', 'æ³•/æ³•æ”¿ç­–',
    'ç¾ä»£ç¤¾ä¼š/ç¾ä»£ç¤¾ä¼š', 'ç¾ä»£ç¤¾ä¼š/å¥åº·ã‚¹ãƒãƒ¼ãƒ„ç¤¾ä¼š',
    'å›½éš›é–¢ä¿‚/å›½éš›é–¢ä¿‚',
    'å¤–å›½èª/è‹±èª', 'å¤–å›½èª/ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘è¨€èª', 'å¤–å›½èª/ã‚¢ã‚¸ã‚¢è¨€èª',
    'æ–‡åŒ–/æ–‡åŒ–æ§‹æƒ³', 'æ–‡åŒ–/äº¬éƒ½æ–‡åŒ–', 'æ–‡åŒ–/æ–‡åŒ–è¦³å…‰'
]

rikei_courses = [
    'ç†/æ•°ç†ç§‘', 'ç†/ç‰©ç†ç§‘', 'ç†/å®‡å®™ç‰©ç†ãƒ»æ°—è±¡',
    'æƒ…å ±ç†å·¥/æƒ…å ±ç†å·¥',
    'ç”Ÿå‘½ç§‘/å…ˆç«¯ç”Ÿå‘½ç§‘', 'ç”Ÿå‘½ç§‘/ç”£æ¥­ç”Ÿå‘½ç§‘'
]

interest_columns = ['æ—…è¡Œ','èª­æ›¸','éŸ³æ¥½','ã‚¹ãƒãƒ¼ãƒ„','æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒ','ã‚²ãƒ¼ãƒ ','ã‚¢ãƒ‹ãƒ¡ãƒ»æ¼«ç”»']
meta_columns = ['æ€§åˆ¥','æ–‡ç†','åå·®å€¤']
character_columns = [
    'ISTJ(ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚·ãƒ£ãƒ³)','ISFJ(æ“è­·è€…)','INFJ(æå”±è€…)','INTJ(å»ºç¯‰å®¶)',
    'ISTP(å·¨åŒ )','ISFP(å†’é™ºå®¶)','INFP(ä»²ä»‹è€…)','INTP(è«–ç†å­¦è€…)',
    'ESTP(èµ·æ¥­å®¶)','ESFP(ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒŠãƒ¼)','ENFP(é‹å‹•å®¶)','ENTP(è¨è«–è€…)',
    'ESTJ(å¹¹éƒ¨)','ESFJ(é ˜äº‹)','ENFJ(ä¸»äººå…¬)','ENTJ(æŒ‡æ®å®˜)'
]
subject_columns = ['å›½èª','æ•°å­¦','è‹±èª','ç†ç§‘','ç¤¾ä¼š']

# ===============================
# UIï¼šé‡ã¿èª¿æ•´
# ===============================
st.sidebar.title("âš™ ãŠã™ã™ã‚æ¡ä»¶ã®èª¿æ•´")

interest_w = st.sidebar.slider("èˆˆå‘³ãƒ»é–¢å¿ƒã®é‡è¦–åº¦", 0.5, 5.0, 5.0)
subject_w  = st.sidebar.slider("å¾—æ„ç§‘ç›®ã®é‡è¦–åº¦", 0.5, 8.0, 8.0)
mbti_w     = st.sidebar.slider("æ€§æ ¼ã‚¿ã‚¤ãƒ—ã®é‡è¦–åº¦", 0.5, 5.0, 5.0)
meta_w     = st.sidebar.slider("åŸºæœ¬å±æ€§ã®é‡è¦–åº¦", 0.1, 2.0, 2.0)

alpha = st.sidebar.slider("å€‹äººç‰¹æ€§ã®é‡è¦–åº¦", 0.0, 1.0, 1.0)
st.sidebar.caption("â† å…ˆè¼©ã®é€²è·¯å‚¾å‘ã€€ï½œã€€ã‚ãªãŸã®ç‰¹å¾´ â†’")


# ===============================
# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
# ===============================
course_df = df[bunkei_courses + rikei_courses]
course_columns = bunkei_courses + rikei_courses
features_df = df[
    interest_columns + meta_columns + character_columns + subject_columns
].copy()

# é‡ã¿é©ç”¨
features_df[interest_columns] *= interest_w
features_df[subject_columns] *= subject_w
features_df[character_columns] *= mbti_w
features_df[meta_columns] *= meta_w

# ===============================
# SVD ãƒ¢ãƒ‡ãƒ«
# ===============================
svd = TruncatedSVD(n_components=5, random_state=42)
latent_user = svd.fit_transform(course_df)
latent_course = svd.components_

def svd_score():
    user_latent = latent_user.mean(axis=0)
    svd_scores = np.dot(user_latent, latent_course)
    return pd.Series(svd_scores, index=course_columns)

# ===============================
# æ¨è–¦é–¢æ•°
# ===============================
def recommend_courses(user_features, bunri, top_n=5):
    user_vec = np.array(user_features).reshape(1, -1)
    user_vec = user_vec / (np.linalg.norm(user_vec) + 1e-8)

    X = features_df.values
    X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)

    similarities = cosine_similarity(user_vec, X)[0]

    # =========================
    # â˜… æº€è¶³åº¦ã‚’é‡ã¿ã¨ã—ã¦é©ç”¨ï¼ˆæ ¸å¿ƒï¼‰
    # =========================
    satisfaction = df["æº€è¶³åº¦"].values  # 1ã€œ5æƒ³å®š
    satisfaction_weight = satisfaction / satisfaction.max()

    weighted_sim = similarities * satisfaction_weight

    # é¡ä¼¼åº¦ãŒå°ã•ã™ãã‚‹ã‚‚ã®é™¤å¤–
    top_k = 50
    top_idx = np.argsort(weighted_sim)[-top_k:]
    top_sim = weighted_sim[top_idx]

    feature_score = (
        np.dot(top_sim, course_df.values[top_idx])
        / (top_sim.sum() + 1e-8)
    )

    feature_score = pd.Series(feature_score, index=course_columns)

    # =========================
    # SVD
    # =========================
    svd_scores = svd_score()

    # =========================
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    # =========================
    final_score = alpha * feature_score + (1 - alpha) * svd_scores

    # =========================
    # æ–‡ç†ãƒ•ã‚£ãƒ«ã‚¿
    # =========================
    if bunri == "æ–‡ç³»":
        final_score = final_score[bunkei_courses]
    else:
        final_score = final_score[rikei_courses]

    return final_score.sort_values(ascending=False).head(top_n)



# ===============================
# UI
# ===============================
st.title("ğŸ“ äº¬ç”£å¤§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é€²è·¯æ¨è–¦")

user_features = []

st.subheader("â‘  èˆˆå‘³")
for col in interest_columns:
    user_features.append((1 if st.checkbox(col) else 0) * interest_w)

st.subheader("â‘¡ åŸºæœ¬æƒ…å ±")
gender = st.selectbox("æ€§åˆ¥", ["ç”·æ€§","å¥³æ€§"])
bunri = st.selectbox("æ–‡ç†", ["æ–‡ç³»","ç†ç³»"])
hensachi = st.slider("åå·®å€¤", 35, 70, 50)

user_features += [
    (0 if gender=="ç”·æ€§" else 1)*meta_w,
    (0 if bunri=="æ–‡ç³»" else 1)*meta_w,
    (hensachi/100)*meta_w
]

st.subheader("â‘¢ MBTI")
mbti = st.selectbox("MBTI", character_columns)
for col in character_columns:
    user_features.append((1 if col==mbti else 0)*mbti_w)

st.subheader("â‘£ å¾—æ„ç§‘ç›®")
kamoku = st.selectbox("å¾—æ„ç§‘ç›®", subject_columns)
for col in subject_columns:
    user_features.append((1 if col==kamoku else 0)*subject_w)

# ===============================
# å®Ÿè¡Œ
# ===============================
if st.button("é€²è·¯ã‚’æ¨è–¦"):
    result = recommend_courses(user_features, bunri, top_n=3)

    st.subheader("ãŠã™ã™ã‚å­¦ç§‘")
    for i, (name, score) in enumerate(result.items(), 1):
        st.markdown(f"### {i}. {name}")
        st.write(f"ã‚¹ã‚³ã‚¢: {score:.2f}")
        st.write("**ç†ç”±ï¼š**")
        st.write("ãƒ»å±æ€§ãŒã‚ãªãŸã¨è¿‘ãã€é€²å­¦å¾Œã®æº€è¶³åº¦ãŒé«˜ã‹ã£ãŸå…ˆè¼©ã®é€²è·¯ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™")
       

