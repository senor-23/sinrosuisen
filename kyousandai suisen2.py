import streamlit as st
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD

# ===============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæº€è¶³åº¦å…¥ã‚Šï¼‰
# ===============================
df = pd.read_excel("excel2.xlsx", sheet_name="Sheet1")

# ===============================
# å­¦ç§‘å®šç¾©
# ===============================
bunkei_courses = [
    'çµŒæ¸ˆ/çµŒæ¸ˆ', 'çµŒå–¶/ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'æ³•/æ³•å¾‹', 'æ³•/æ³•æ”¿ç­–',
    'ç¾ä»£ç¤¾ä¼š/ç¾ä»£ç¤¾ä¼š', 'ç¾ä»£ç¤¾ä¼š/å¥åº·ã‚¹ãƒãƒ¼ãƒ„ç¤¾ä¼š',
    'å›½éš›é–¢ä¿‚/å›½éš›é–¢ä¿‚',
    'å¤–å›½èª/è‹±èª', 'å¤–å›½èª/ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘è¨€èª', 'å¤–å›½èª/ã‚¢ã‚¸ã‚¢è¨€èª',
    'æ–‡åŒ–/æ–‡åŒ–æ§‹æƒ³', 'æ–‡åŒ–/äº¬éƒ½æ–‡åŒ–', 'æ–‡åŒ–/æ–‡åŒ–è¦³å…‰'
]

rikei_courses = [
    'ç†/æ•°ç†ç§‘', 'ç†/ç‰©ç†ç§‘', 'ç†/å®‡å®™ç‰©ç†ãƒ»æ°—è±¡',
    'æƒ…å ±ç†å·¥/æƒ…å ±ç†å·¥',
    'ç”Ÿå‘½ç§‘/å…ˆç«¯ç”Ÿå‘½ç§‘', 'ç”Ÿå‘½ç§‘/ç”£æ¥­ç”Ÿå‘½ç§‘'
]

course_columns = bunkei_courses + rikei_courses

# ===============================
# ç‰¹å¾´é‡å®šç¾©
# ===============================
interest_columns = ['æ—…è¡Œ','èª­æ›¸','éŸ³æ¥½','ã‚¹ãƒãƒ¼ãƒ„','æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒ','ã‚²ãƒ¼ãƒ ','ã‚¢ãƒ‹ãƒ¡ãƒ»æ¼«ç”»']
meta_columns = ['æ€§åˆ¥','æ–‡ç†','åå·®å€¤']
character_columns = [
    'ISTJ(ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚·ãƒ£ãƒ³)','ISFJ(æ“è­·è€…)','INFJ(æå”±è€…)','INTJ(å»ºç¯‰å®¶)',
    'ISTP(å·¨åŒ )','ISFP(å†’é™ºå®¶)','INFP(ä»²ä»‹è€…)','INTP(è«–ç†å­¦è€…)',
    'ESTP(èµ·æ¥­å®¶)','ESFP(ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒŠãƒ¼)','ENFP(é‹å‹•å®¶)','ENTP(è¨è«–è€…)',
    'ESTJ(å¹¹éƒ¨)','ESFJ(é ˜äº‹)','ENFJ(ä¸»äººå…¬)','ENTJ(æŒ‡æ®å®˜)'
]
subject_columns = ['å›½èª','æ•°å­¦','è‹±èª','ç†ç§‘','ç¤¾ä¼š']

# ===============================
# UIï¼šé‡ã¿èª¿æ•´
# ===============================
st.sidebar.title("âš™ é‡ã¿èª¿æ•´")

interest_w = st.sidebar.slider("èˆˆå‘³ã®é‡ã¿", 0.5, 5.0, 3.0)
subject_w  = st.sidebar.slider("å¾—æ„ç§‘ç›®ã®é‡ã¿", 0.5, 8.0, 5.0)
mbti_w     = st.sidebar.slider("MBTIã®é‡ã¿", 0.5, 5.0, 2.0)
meta_w     = st.sidebar.slider("å±æ€§ã®é‡ã¿", 0.1, 2.0, 1.0)
alpha      = st.sidebar.slider("æº€è¶³åº¦ vs SVD", 0.0, 1.0, 0.8)

# ===============================
# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
# ===============================
course_df = df[course_columns]  # â† æº€è¶³åº¦ï¼ˆ1ã€œ10 or NaNï¼‰

features_df = df[
    interest_columns + meta_columns + character_columns + subject_columns
].copy()

# é‡ã¿åæ˜ ï¼ˆå­¦ç¿’å´ï¼‰
features_df[interest_columns]  *= interest_w
features_df[subject_columns]   *= subject_w
features_df[character_columns] *= mbti_w
features_df[meta_columns]      *= meta_w

# ===============================
# SVDï¼ˆè£œåŠ©ï¼‰
# ===============================
svd = TruncatedSVD(n_components=5, random_state=42)
latent_user = svd.fit_transform(course_df.fillna(0))
latent_course = svd.components_

def svd_score():
    user_latent = latent_user.mean(axis=0)
    scores = np.dot(user_latent, latent_course)
    return pd.Series(scores, index=course_columns)

# ===============================
# æ¨è–¦ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ ¸å¿ƒï¼‰
# ===============================
def recommend_courses(user_features, bunri, top_n=5):
    # ---- é¡ä¼¼åº¦ ----
    user_vec = np.array(user_features).reshape(1, -1)
    user_vec = user_vec / (np.linalg.norm(user_vec) + 1e-8)

    X = features_df.values
    X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)

    similarities = cosine_similarity(user_vec, X)[0]

    # ---- æº€è¶³åº¦ Ã— é¡ä¼¼åº¦ ----
    sim = similarities.reshape(-1, 1)
    weighted_satisfaction = sim * course_df.values

    satisfaction_score = np.nanmean(weighted_satisfaction, axis=0)
    satisfaction_score = pd.Series(satisfaction_score, index=course_columns)

    # ---- SVD ----
    svd_scores = svd_score()

    # ---- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ ----
    final_score = alpha * satisfaction_score + (1 - alpha) * svd_scores

    # ---- æ–‡ç†ãƒ•ã‚£ãƒ«ã‚¿ ----
    if bunri == "æ–‡ç³»":
        final_score = final_score[bunkei_courses]
    else:
        final_score = final_score[rikei_courses]

    return final_score.sort_values(ascending=False).head(top_n)

# å­¦éƒ¨ã”ã¨ã®å¹³å‡ã‚’è¨ˆç®—
faculty_mean = course_df.groupby(faculty_map, axis=1).mean()

# å­¦éƒ¨å¹³å‡ã¨ã®å·®ã‚’å¼•ã
course_debiased = course_centered.copy()
for course in course_centered.columns:
    faculty = faculty_map[course]
    course_debiased[course] -= faculty_mean[faculty]


# ===============================
# UI
# ===============================
st.title("ğŸ“ äº¬ç”£å¤§ é€²è·¯æ¨è–¦ï¼ˆæº€è¶³åº¦é‡è¦–ï¼‰")

user_features = []

st.subheader("â‘  èˆˆå‘³")
for col in interest_columns:
    user_features.append((1 if st.checkbox(col) else 0) * interest_w)

st.subheader("â‘¡ åŸºæœ¬æƒ…å ±")
gender = st.selectbox("æ€§åˆ¥", ["ç”·æ€§","å¥³æ€§"])
bunri  = st.selectbox("æ–‡ç†", ["æ–‡ç³»","ç†ç³»"])
hensachi = st.slider("åå·®å€¤", 35, 70, 50)

user_features += [
    (0 if gender=="ç”·æ€§" else 1) * meta_w,
    (0 if bunri=="æ–‡ç³»" else 1) * meta_w,
    (hensachi / 100) * meta_w
]

st.subheader("â‘¢ MBTI")
mbti = st.selectbox("MBTI", character_columns)
for col in character_columns:
    user_features.append((1 if col == mbti else 0) * mbti_w)

st.subheader("â‘£ å¾—æ„ç§‘ç›®")
kamoku = st.selectbox("å¾—æ„ç§‘ç›®", subject_columns)
for col in subject_columns:
    user_features.append((1 if col == kamoku else 0) * subject_w)

# ===============================
# å®Ÿè¡Œ
# ===============================
if st.button("é€²è·¯ã‚’æ¨è–¦"):
    result = recommend_courses(user_features, bunri, top_n=3)

    st.subheader("ğŸŒŸ ãŠã™ã™ã‚å­¦ç§‘")
    for i, (name, score) in enumerate(result.items(), 1):
        st.markdown(f"### {i}. {name}")
        st.write(f"ã‚¹ã‚³ã‚¢: {score:.2f}")
        st.write("**ç†ç”±ï¼š**")
        st.write("ãƒ»ã‚ãªãŸã¨ä¼¼ãŸå­¦ç”Ÿã®æº€è¶³åº¦ãŒé«˜ã„")
        st.write("ãƒ»èˆˆå‘³ãƒ»å¾—æ„ç§‘ç›®ãƒ»å‚¾å‘ãŒä¸€è‡´")
